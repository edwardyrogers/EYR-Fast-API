{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from numpy import mean\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embed_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "# embed_model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "# embed_model = SentenceTransformer('sentence-transformers/gtr-t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_topic(\n",
    "    new_msg: str,\n",
    "    ctx_list: List[str],\n",
    "    decay_rate: float = 0.85,\n",
    "    max_threshold: float = 0.85,\n",
    "    mean_threshold: float = 0.75,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    if not ctx_list: return True\n",
    "\n",
    "    ctx_emb = embed_model.encode(ctx_list, convert_to_tensor=True)\n",
    "    msg_emb = embed_model.encode(new_msg, convert_to_tensor=True)\n",
    "\n",
    "    similarities = util.cos_sim(msg_emb, ctx_emb)[0]\n",
    "\n",
    "    weights = [decay_rate**i for i in reversed(range(len(ctx_list)))]\n",
    "    weighted_similarities = [sim.item() * w for sim, w in zip(similarities, weights)]\n",
    "\n",
    "    max_sim = max(weighted_similarities)\n",
    "    mean_sim = mean(weighted_similarities)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Current message: {new_msg}\")\n",
    "        print(f\"Weighted Similarities\")\n",
    "\n",
    "        for i, (msg, score) in enumerate(zip(ctx_list, weighted_similarities)):\n",
    "            print(f\" [{i}] {msg[:40]}... -> {score:.4f}\")\n",
    "\n",
    "        print(f\"\\n Max Sim: {max_sim:.4f}, Mean Sim: {mean_sim:.4f}\")\n",
    "        print(\n",
    "            f\" Decision: {'In Context' if (max_sim > max_threshold and mean_sim > mean_threshold) else 'New Topic'}\\n\"\n",
    "        )\n",
    "\n",
    "    return max_sim > max_threshold and mean_sim > mean_threshold\n",
    "\n",
    "# Simulate message input\n",
    "messages = [\n",
    "    # \"我上星期去了台北玩。\",\n",
    "    # \"天氣很熱，但是很好玩。\",\n",
    "    # \"我還去了故宮，裡面有很多古董。\",\n",
    "    \n",
    "    # \"下週學校要開始期中考了。\",\n",
    "    # \"我數學還沒讀完，有點緊張。\",\n",
    "    # \"最近睡不好，常常熬夜。\",\n",
    "    # \"醫生建議我早點睡，多運動。\",\n",
    "    \n",
    "    # \"我昨天遇到小美。\",\n",
    "    # \"她說她也去了台北出差。\",\n",
    "    \n",
    "    \"今天天氣怎麼樣？\",\n",
    "    \"今天天氣很好，陽光明媚。\",\n",
    "    \"是啊，這樣的天氣真讓人心情愉快。\",\n",
    "    \"對，適合外出走走。\",\n",
    "    \n",
    "    \"你最近有去哪裡旅行嗎？\",\n",
    "    \"有，我去了京都。\",\n",
    "    \"哇，那裡怎麼樣？\",\n",
    "    \"非常美麗，特別是秋天的紅葉。\",\n",
    "    \n",
    "    \"你最近身體怎麼樣？\",\n",
    "    \"還不錯，就是有點累。\",\n",
    "    \"那你有去運動嗎？\",\n",
    "    \"有，我每天早上跑步。\",\n",
    "    \n",
    "    \"你最近看過什麼好電影嗎？\",\n",
    "    \"有，我看了《流浪地球》。\",\n",
    "    \"怎麼樣？好看嗎？\",\n",
    "    \"非常精彩，特效做得很棒。\",\n",
    "]\n",
    "\n",
    "conversation = []\n",
    "\n",
    "for message in messages:\n",
    "    res = is_in_topic(message, conversation)\n",
    "    \n",
    "    if (res):\n",
    "        conversation.append(message)\n",
    "    else:\n",
    "        conversation = [message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbaa365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Step 1: Define intent labels and mappings\n",
    "labels = [\"store_in_stm\", \"store_in_ltm\", \"retrieve_stm\", \"retrieve_ltm\", \"none\"]\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Step 2: Load training data\n",
    "raw_dataset = load_dataset(\"csv\", data_files=\"./models/intent_classifier/training_data.csv\")\n",
    "training_dataset = raw_dataset[\"train\"].to_list()\n",
    "\n",
    "# Convert labels to IDs\n",
    "for data in training_dataset:\n",
    "    data[\"label\"] = label2id[data[\"label\"]]\n",
    "\n",
    "# Step 3: Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_list(training_dataset)\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Step 4: Loading\n",
    "model_name = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Step 5: Tokenize\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "tokenized = train_test.map(tokenize)\n",
    "\n",
    "\n",
    "# Step 6: Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/intent_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "\n",
    "# Optional: Evaluation function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "\n",
    "# Step 7: Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./models/intent_classifier/model\")\n",
    "tokenizer.save_pretrained(\"./models/intent_classifier/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model and tokenizer\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"./models/intent_classifier/model\",\n",
    "    tokenizer=\"./models/intent_classifier/model\",\n",
    "    return_all_scores=False  # Set to True if you want all class probabilities\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37108b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier(\"請暫時儲存這個地址\")[0]\n",
    "print(f\"Intent: {result['label']} (Confidence: {result['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443223fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Step 1: Define intent labels and mappings\n",
    "labels = [\"WORKING\", \"SEMANTIC\", \"PERSONAL\"]\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Step 2: Load training data\n",
    "raw_dataset = load_dataset(\n",
    "    \"csv\", data_files=\"./models/mem_type_classifier/training_data.csv\"\n",
    ")\n",
    "training_dataset = raw_dataset[\"train\"].to_list()\n",
    "\n",
    "# Convert labels to IDs\n",
    "for data in training_dataset:\n",
    "    data[\"label\"] = label2id[data[\"label\"]]\n",
    "\n",
    "# Step 3: Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_list(training_dataset)\n",
    "train_test = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "# Step 4: Loading\n",
    "model_name = \"ckiplab/bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Tokenize\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "tokenized = train_test.map(tokenize)\n",
    "\n",
    "\n",
    "# Step 6: Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/mem_type_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Optional: Evaluation function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "\n",
    "# Step 7: Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./models/mem_type_classifier/model\")\n",
    "tokenizer.save_pretrained(\"./models/mem_type_classifier/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model and tokenizer\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"./models/mem_type_classifier/model\",\n",
    "    tokenizer=\"./models/mem_type_classifier/model\",\n",
    "    return_all_scores=False  # Set to True if you want all class probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "text = \"台灣總統是誰？\"\n",
    "result = classifier(text)[0]\n",
    "\n",
    "print(f\"Intent: {result['label']} (Confidence: {result['score']:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
