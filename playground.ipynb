{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from numpy import mean\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embed_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "# embed_model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "# embed_model = SentenceTransformer('sentence-transformers/gtr-t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_topic(\n",
    "    new_msg: str,\n",
    "    ctx_list: List[str],\n",
    "    decay_rate: float = 0.85,\n",
    "    max_threshold: float = 0.85,\n",
    "    mean_threshold: float = 0.75,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    if not ctx_list: return True\n",
    "\n",
    "    ctx_emb = embed_model.encode(ctx_list, convert_to_tensor=True)\n",
    "    msg_emb = embed_model.encode(new_msg, convert_to_tensor=True)\n",
    "\n",
    "    similarities = util.cos_sim(msg_emb, ctx_emb)[0]\n",
    "\n",
    "    weights = [decay_rate**i for i in reversed(range(len(ctx_list)))]\n",
    "    weighted_similarities = [sim.item() * w for sim, w in zip(similarities, weights)]\n",
    "\n",
    "    max_sim = max(weighted_similarities)\n",
    "    mean_sim = mean(weighted_similarities)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Current message: {new_msg}\")\n",
    "        print(f\"Weighted Similarities\")\n",
    "\n",
    "        for i, (msg, score) in enumerate(zip(ctx_list, weighted_similarities)):\n",
    "            print(f\" [{i}] {msg[:40]}... -> {score:.4f}\")\n",
    "\n",
    "        print(f\"\\n Max Sim: {max_sim:.4f}, Mean Sim: {mean_sim:.4f}\")\n",
    "        print(\n",
    "            f\" Decision: {'In Context' if (max_sim > max_threshold and mean_sim > mean_threshold) else 'New Topic'}\\n\"\n",
    "        )\n",
    "\n",
    "    return max_sim > max_threshold and mean_sim > mean_threshold\n",
    "\n",
    "# Simulate message input\n",
    "messages = [\n",
    "    # \"我上星期去了台北玩。\",\n",
    "    # \"天氣很熱，但是很好玩。\",\n",
    "    # \"我還去了故宮，裡面有很多古董。\",\n",
    "    \n",
    "    # \"下週學校要開始期中考了。\",\n",
    "    # \"我數學還沒讀完，有點緊張。\",\n",
    "    # \"最近睡不好，常常熬夜。\",\n",
    "    # \"醫生建議我早點睡，多運動。\",\n",
    "    \n",
    "    # \"我昨天遇到小美。\",\n",
    "    # \"她說她也去了台北出差。\",\n",
    "    \n",
    "    \"今天天氣怎麼樣？\",\n",
    "    \"今天天氣很好，陽光明媚。\",\n",
    "    \"是啊，這樣的天氣真讓人心情愉快。\",\n",
    "    \"對，適合外出走走。\",\n",
    "    \n",
    "    \"你最近有去哪裡旅行嗎？\",\n",
    "    \"有，我去了京都。\",\n",
    "    \"哇，那裡怎麼樣？\",\n",
    "    \"非常美麗，特別是秋天的紅葉。\",\n",
    "    \n",
    "    \"你最近身體怎麼樣？\",\n",
    "    \"還不錯，就是有點累。\",\n",
    "    \"那你有去運動嗎？\",\n",
    "    \"有，我每天早上跑步。\",\n",
    "    \n",
    "    \"你最近看過什麼好電影嗎？\",\n",
    "    \"有，我看了《流浪地球》。\",\n",
    "    \"怎麼樣？好看嗎？\",\n",
    "    \"非常精彩，特效做得很棒。\",\n",
    "]\n",
    "\n",
    "conversation = []\n",
    "\n",
    "for message in messages:\n",
    "    res = is_in_topic(message, conversation)\n",
    "    \n",
    "    if (res):\n",
    "        conversation.append(message)\n",
    "    else:\n",
    "        conversation = [message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbaa365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Define intent labels and mappings\n",
    "labels = [\"store_in_stm\", \"store_in_ltm\", \"retrieve_stm\", \"retrieve_ltm\", \"none\"]\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Step 2: Define example Chinese data\n",
    "examples = [\n",
    "    {\"text\": \"暂时记住：我今天下午要开会\", \"label\": \"store_in_stm\"},\n",
    "    {\"text\": \"记住我现在的心情是开心的\", \"label\": \"store_in_stm\"},\n",
    "    {\"text\": \"请记得：我住在北京\", \"label\": \"store_in_ltm\"},\n",
    "    {\"text\": \"我的生日是5月1日，请记住\", \"label\": \"store_in_ltm\"},\n",
    "    {\"text\": \"你还记得我刚才说什么吗？\", \"label\": \"retrieve_stm\"},\n",
    "    {\"text\": \"我们之前聊过的内容是什么？\", \"label\": \"retrieve_stm\"},\n",
    "    {\"text\": \"你知道我喜欢什么电影吗？\", \"label\": \"retrieve_ltm\"},\n",
    "    {\"text\": \"你还记得我家的地址吗？\", \"label\": \"retrieve_ltm\"},\n",
    "    {\"text\": \"给我讲个笑话\", \"label\": \"none\"},\n",
    "    {\"text\": \"今天的天气怎么样？\", \"label\": \"none\"}\n",
    "]\n",
    "\n",
    "# Convert labels to IDs\n",
    "for ex in examples:\n",
    "    ex[\"label\"] = label2id[ex[\"label\"]]\n",
    "\n",
    "# Step 3: Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_list(examples)\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Step 4: Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized = train_test.map(tokenize)\n",
    "\n",
    "# Step 5: Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-chinese\",\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Step 6: Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Optional: Evaluation function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "    }\n",
    "\n",
    "# Step 7: Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37108b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Predict on a new sentence\n",
    "result = classifier(\"请把这条信息存起来：我不喜欢吃辣的\")\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
